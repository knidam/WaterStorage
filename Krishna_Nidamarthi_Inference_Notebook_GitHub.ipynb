{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92000df-fb7f-4daf-82de-5552608a34aa",
   "metadata": {},
   "source": [
    "# Model Inferencing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a8f04",
   "metadata": {},
   "source": [
    "## Research Title: Utilizing AI and IoT for Climate Adaptation\n",
    "### Subtitle: Forecasting, Directing, and Storing Extreme Precipitation Events for Flood Prevention and Water Security\n",
    "\n",
    "### By Krishna Nidamarthi, Emerald High School, Dublin CA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2d923",
   "metadata": {},
   "source": [
    "#### About this research:\n",
    "\n",
    "#### Abstract\n",
    "\n",
    "Climate change is intensifying extreme precipitation events, leading to increased flooding while alternatively creating longer drought periods that threaten water availability. This research presents an integrated approach to adapt to this climate challenge in the short term by combining climate modeling, artificial intelligence & machine learning (AIML), Internet of Things (IoT) and cloud technologies. The study developed a three-pronged solution: (1) high-resolution precipitation forecast analysis with downscaling, (2) AI-powered satellite image analysis to rapidly identify potential water storage areas, and (3) IoT-enabled dynamic water management infrastructure using remote monitoring and control via the cloud. Using Merced County, California as a test case, the research employed the MACA-CCSM4 downscaling datasets to forecast future precipitation patterns. The analysis revealed a 150% increase in extreme precipitation over Sierra Nevada for 2026-2050 and 250% for 2051-2099 compared to 2000-2024 which escalated flood risk for Merced County. A custom convolutional neural network using Resnet50 with Feature Pyramid Network (FPN) was developed to analyze satellite imagery to identify suitable water reservoir areas. The proposed solution includes engineered wetlands and methods for constructing them in the identified reservoir areas, incorporating cloud-connected IoT sensors and remotely operated controls. These wetlands are capable of storing up to 2.5x106 cubic meters of surface water. This comprehensive approach demonstrates the potential of technology-driven climate adaptation strategies to simultaneously address flood prevention and water security challenges in vulnerable regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff33081",
   "metadata": {},
   "source": [
    "#### About this Jupyter Notebook\n",
    "\n",
    "This notebook is the accompanying ML model inference code and it used the ML model that I trained as part of my research. The model and sample images are available in my Github. The model is a resnet50 CNN trained model, with FPN, and used to classify Google maps satellite tiles (zoom level 12, 512x512 pixels resolution [at scale 2], and without cloud cover) for 1) Farms, 2) Lakes, 3) Reservoir-Areas, and 4) Suburbs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76605c-b030-42bc-a888-762a64fbc0c0",
   "metadata": {},
   "source": [
    "Libraries and initialization. The Python environment in Jupyter lab must be setup with the following libraries.\n",
    "With regards to the compatibility I used: pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "7fe5a0ab-4a42-4747-a8cc-feea93449b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3ed40",
   "metadata": {},
   "source": [
    "I use CVAT 1.1 for annotations and the following are the classes (labels) identified in a given Google maps satellite tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d2c17-b7d9-4afe-abbd-060042ec0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAT label configuration\n",
    "LABEL_CONFIG = {\n",
    "    'Farm': '#d08e1f',\n",
    "    'Lake': '#93e4ca',\n",
    "    'Reservoir-Area': '#3c3ffc',\n",
    "    'Suburb': '#ff355e'\n",
    "}\n",
    "\n",
    "# GLOBAL CONSTANTS\n",
    "DEBUG_MODE = False # Set to False for normal execution\n",
    "WARN_MODE = False # False to suppress warning printouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e8d77-ac02-41a7-9c28-f95cf65fc806",
   "metadata": {},
   "source": [
    "Tensor class compoistion (same as training module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72825a90-7d04-4520-948c-d621ec3a4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512656bf-0712-4a8f-a355-53082b511243",
   "metadata": {},
   "source": [
    "BoxRCNN class from training (same class as defined in training notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1866f6-40f7-462f-bc7a-192208c92648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxRCNN(torchvision.models.detection.MaskRCNN):\n",
    "    def __init__(self, num_classes, class_to_idx):\n",
    "        \n",
    "        # Load a pre-trained ResNet50 model\n",
    "        backbone = resnet50(weights=ResNet50_Weights.DEFAULT)        \n",
    "        # Use the first 4 stages of ResNet\n",
    "        backbone = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        # Define the return layers for FPN\n",
    "        return_layers = {'4': '0', '5': '1', '6': '2', '7': '3'}\n",
    "        \n",
    "        # Create FPN\n",
    "        backbone_with_fpn = BackboneWithFPN(\n",
    "            backbone,\n",
    "            return_layers,\n",
    "            in_channels_list=[256, 512, 1024, 2048],\n",
    "            out_channels=256\n",
    "        )\n",
    "        # Initialize the MaskRCNN with our custom backbone\n",
    "        super(BoxRCNN, self).__init__(backbone_with_fpn, num_classes)\n",
    "\n",
    "        # Store class mappings\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "        \n",
    "        # Replace the box predictor\n",
    "        in_features = self.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        \n",
    "        # Replace the mask predictor\n",
    "        in_features_mask = self.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        self.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "        #self.idx_to_class[0] = 'background' # explicitly additional background class\n",
    "        #background class is area in the image where there is no classified image. i.e, BACKGROUND.\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # For training mode\n",
    "        if self.training and targets is not None:\n",
    "            # Keep original string labels for later comparison\n",
    "            for t in targets:\n",
    "                 # Check if labels exist and are not empty\n",
    "                if 'labels' not in t or len(t['labels']) == 0:\n",
    "                    if TRAIN_DEBUG: print(\"Warning: Empty labels in target\")\n",
    "                    # Initialize with empty tensors\n",
    "                    t['labels'] = torch.zeros(0, dtype=torch.int64, device=images[0].device)\n",
    "                    continue\n",
    "                \n",
    "                if len(t['labels']) > 0 and isinstance(t['labels'][0], str):\n",
    "                    try:\n",
    "                         t['labels'] = torch.tensor([self.class_to_idx[label] for label in t['labels']], \n",
    "                                                    dtype=torch.int64, device=images[0].device)\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Error: Unknown label encountertered: {e}\")\n",
    "                        print(f\"Available classes: {self.class_to_idx.keys()}\")\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting labels: {str(e)}\")\n",
    "                        print(f\"Labels: {t['labels']}\")\n",
    "                        raise\n",
    "        \n",
    "        # MaskRCNN forward pass (works with indices)\n",
    "        outputs = super(BoxRCNN, self).forward(images, targets)\n",
    "\n",
    "        # Convert back to strings for external use if not in training mode\n",
    "        if not self.training:\n",
    "            for output in outputs:\n",
    "                if 'labels' not in output or len(output['labels']) == 0:\n",
    "                    if DEBUG_MODE: print(\"No detections in MaskRCNN forward pass output\")\n",
    "                    output['labels'] = []\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    output['labels'] = [self.idx_to_class[label.item()] \n",
    "                                        if isinstance(label, torch.Tensor)\n",
    "                                        else self.idx_to_class[label]\n",
    "                                        for label in output['labels']]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting MaskRCNN forward pass output labels: {str(e)}\")\n",
    "                    print(f\"Output labels: {output['labels']}\")\n",
    "                    raise\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def detect(self, images):\n",
    "        # Helper method for inference\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self(images)  # This calls the forward method\n",
    "\n",
    "            if TRAIN_DEBUG: print('In BoxRCNN.detect: ')\n",
    "            \n",
    "            for pred in predictions:\n",
    "                # Check if labels exist and are not empty\n",
    "                if 'labels' not in pred or len(pred['labels']) == 0:\n",
    "                    if TRAIN_DEBUG:\n",
    "                        print(\"No detections in this image\")\n",
    "                    # Initialize with empty lists/tensors\n",
    "                    pred['boxes'] = torch.empty((0, 4), device=pred['boxes'].device)\n",
    "                    pred['labels'] = []\n",
    "                    pred['scores'] = torch.empty(0, device=pred['scores'].device)\n",
    "                    pred['masks'] = torch.empty((0, 1, pred['masks'].shape[2], pred['masks'].shape[3]), \n",
    "                                             device=pred['masks'].device)\n",
    "                    continue\n",
    "                \n",
    "                # Convert labels to strings if they aren't already\n",
    "                if not isinstance(pred['labels'][0], str):\n",
    "                    pred['labels'] = [self.idx_to_class[label.item()] \n",
    "                                    if isinstance(label, torch.Tensor) \n",
    "                                    else self.idx_to_class[label] \n",
    "                                    for label in pred['labels']]\n",
    "                \n",
    "                if TRAIN_DEBUG:\n",
    "                    print(\"In BoxRCNN.detect:\")\n",
    "                    print(f\"Number of detections: {len(pred['labels'])}\")\n",
    "                    print(f\"Labels: {pred['labels']}\")\n",
    "                    print(f\"Scores: {pred['scores']}\")\n",
    "                    print(f\"Label types: {[type(label) for label in pred['labels']]}\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0307b95-4bfe-4601-a118-a4e90e40f69b",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4036-6310-41c6-a659-9c7508ea1d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# color for bounding boxes\n",
    "def hex_to_rgb(hex_color: str) -> Tuple[int, int, int]:\n",
    "    \"\"\"Convert hex color to RGB tuple\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1d330-33f9-42bc-9f75-aec38895566d",
   "metadata": {},
   "source": [
    "Check CUDA available, and load ML Model to the appropriate device. CUDA is preferred if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbddcf4-5357-42d2-9924-cc1c47f1f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        # Check the number of available CUDA devices\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 0:\n",
    "            # If multiple devices are available, try to find the NVIDIA GPU\n",
    "            for i in range(device_count):\n",
    "                device_name = torch.cuda.get_device_name(i)\n",
    "                if 'NVIDIA' in device_name or 'RTX' in device_name:\n",
    "                    print(f\"Using CUDA device: {torch.cuda.get_device_name(i)}\")\n",
    "                    return torch.device(f'cuda:{i}')\n",
    "            \n",
    "            # If NVIDIA GPU not found, use the first available CUDA device\n",
    "            print(f\"No NVIDIA GPU, but using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "            return torch.device('cuda:0')\n",
    "    \n",
    "    # If no CUDA devices are available, use CPU\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e16eb1",
   "metadata": {},
   "source": [
    "Load ML Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877a7bc-dcdc-427d-9752-97d014085421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_model(checkpoint_path: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Load model from checkpoint file\n",
    "    Args:\n",
    "        checkpoint_path: Path to the .pth checkpoint file\n",
    "    Returns:\n",
    "        Loaded model\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "    \n",
    "    # Initialize model with correct number of classes (4 classes + background)\n",
    "    model = BoxRCNN(num_classes=5, class_to_idx={k: i+1 for i, k in enumerate(LABEL_CONFIG.keys())})\n",
    "    \n",
    "    # Load state dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        # This is a training checkpoipoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    else:\n",
    "        # This is just the state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Loaded model state dict\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ed3ef-a4a2-45b3-bbc9-ad914e6bac31",
   "metadata": {},
   "source": [
    "Process and visualize satellite image for feature recognition. In the code below, change \"confidence_threshold\" to higher number if you want the inference to be very strict in classifying images, or low value if you want to see if you want to recognize more. Note: In my research, I have a following step of verifying the ML inference results with terrain and elevation data (this verification is not in the scope of this Jupyter notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6617e6-f07d-4af4-b3be-f95c7c4ceda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path: str, model: torch.nn.Module, device: torch.device, \n",
    "                        confidence_threshold: float = 0.25) -> Tuple[torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Process a single image and return predictions\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = F.to_tensor(image)\n",
    "    image_tensor = F.normalize(image_tensor, \n",
    "                             mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    mask = prediction[0]['scores'] > confidence_threshold\n",
    "    boxes = prediction[0]['boxes'][mask]\n",
    "    labels = [prediction[0]['labels'][i] for i in range(len(prediction[0]['scores'])) \n",
    "             if prediction[0]['scores'][i] > confidence_threshold]\n",
    "    scores = prediction[0]['scores'][mask]\n",
    "    \n",
    "    return image, boxes.cpu(), labels, scores.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68ecbc-3ec2-4e67-83a4-e65ac3a4c892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_image_pair(image_path: str, model: torch.nn.Module, device: torch.device):\n",
    "    \"\"\"\n",
    "    Display original and annotated versions of an image side by side\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Load and display original image\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    ax1.imshow(original_image)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Get predictions and display annotated image\n",
    "    image, boxes, labels, scores = process_single_image(image_path, model, device)\n",
    "    \n",
    "    # Display annotated image\n",
    "    ax2.imshow(original_image)\n",
    "    \n",
    "    # Add bounding boxes and labels\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        color = LABEL_CONFIG[label]\n",
    "        rgb_color = hex_to_rgb(color)\n",
    "        \n",
    "        # Create rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]),\n",
    "            box[2] - box[0],\n",
    "            box[3] - box[1],\n",
    "            linewidth=2,\n",
    "            edgecolor=color,\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "        # Add label text\n",
    "        ax2.text(\n",
    "            box[0], box[1]-5,\n",
    "            f'{label}: {score:.2f}',\n",
    "            color=color,\n",
    "            fontsize=8,\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
    "        )\n",
    "    \n",
    "    ax2.set_title('Annotated Image')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f8616",
   "metadata": {},
   "source": [
    "Chang the code below for the notebook to access the ML model stored in your local machine. Insert the path where you dowloaded and stored the ML model in place of \"YOUR LOCAL DIRECTORY FOR ML MODEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4458f7c-91d9-4a29-a16d-d001bf446d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA RTX 500 Ada Generation Laptop GPU\n",
      "Loaded model state dict\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "#FINAL MODEL\n",
    "checkpoint_path = r'YOUR LOCAL DIRECTORY FOR ML MODEL/rcnn_model_2025-02-10_19.pth'\n",
    "model, device = load_checkpoint_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1264e",
   "metadata": {},
   "source": [
    "Chang the code below for the notebook to access the Google satellite tile images stored in your local machine. Insert the path where you dowloaded and stored the sample images, or the images you downloaded using google earth api, in place of \"YOUR LOCAL DIRECTORY FOR IMAGES\". \n",
    "\n",
    "Note: As explained before, this ML inference works with google map tiles at zoom level 12, 512x512 pixels resolution [at scale 2], and without cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5680bed-bf2a-4ec4-9aeb-109d08c966ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize image\n",
    "img_file = [\"\"]*12\n",
    "# Lakes or Rivers\n",
    "img_file[0] = \"NormColCorr_tile_667_1564_-121.38_39.10.png\" # lake & Reservoir-area\n",
    "img_file[1] = \"NormColCorr_tile_670_1546_-121.11_40.31.png\"\n",
    "img_file[2] = \"NormColCorr_tile_688_1569_-119.53_38.75.png\" # lake & farm\n",
    "#farm\n",
    "img_file[3] = \"NormColCorr_tile_667_1521_-121.38_41.97.png\" \n",
    "img_file[4] = \"NormColCorr_tile_670_1589_-121.11_37.37.png\" # Farm, Suburb & Reservoir-area\n",
    "img_file[5] = \"NormColCorr_tile_671_1587_-121.03_37.51.png\"\n",
    "# Reservoir area\n",
    "img_file[6] = \"NormColCorr_tile_676_1526_-120.59_41.64.png\"\n",
    "img_file[7] = \"NormColCorr_tile_708_1585_-117.77_37.65.png\"\n",
    "img_file[8] = \"NormColCorr_tile_708_1618_-117.77_35.32.png\"\n",
    "#Suburb\n",
    "img_file[9] = \"NormColCorr_tile_659_1586_-122.08_37.58.png\"\n",
    "img_file[10] = \"NormColCorr_tile_666_1583_-121.46_37.79.png\" # suburb & farm\n",
    "img_file[11] = \"NormColCorr_tile_667_1572_-121.38_38.55.png\"\n",
    "\n",
    "index_img = 0 # CHANGE THIS INDEX TO TRY MULTIPLE IMAGES.\n",
    "\n",
    "image_path = r'YOUR LOCAL DIRECTORY FOR IMAGES'\n",
    "image_path = os.path.join(image_path, img_file[index_img]) # Change index_img\n",
    "visualize_image_pair(image_path, model, device) \n",
    "\n",
    "\n",
    "## ORIGINAL IMAGE AND ANNOTATED IMAGE WILL BE SHOWN BELOW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
